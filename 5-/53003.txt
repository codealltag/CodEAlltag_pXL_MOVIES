Hi,

kennt einer von euch das Gefühl, dass die eigene Weltanschauung nach einem
guten Film verändert wird. Ich sehe die Welt um mich herum viel positiver,
nachdem ich
mir American Beauty von Urban Kirchhöbel angesehen habe. Es ist fast so, als ob
die Lösung aller Probleme
in diesem Film kompakt gegeben wird.

Empfindet das einer von euch ebenso?